import os
from typing import List, Optional
import gradio as gr

# --- LangChain & Vector Store ---
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# Manejo robusto de importaciones para Embeddings
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

# --- CONFIGURACIÃ“N ---
# Usamos un modelo MULTILINGÃœE. Crucial para que entienda espaÃ±ol semÃ¡ntico.
# 'paraphrase-multilingual-MiniLM-L12-v2' es el estÃ¡ndar "bueno, bonito y barato" para CPU.
MODELO_EMBEDDING = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# Umbral de similitud (Distancia L2 o Coseno). Si la distancia es alta, el doc no es relevante.
# En Chroma (L2), menor distancia = mayor similitud. Ajustar segÃºn pruebas.
UMBRAL_CORTE = 1.5 * 40

class MotorRAG:
    """Clase que encapsula la lÃ³gica del RAG para mantener el estado y la limpieza."""
    
    def __init__(self):
        self.vector_db = self._inicializar_base_conocimiento()

    def _inicializar_base_conocimiento(self) -> Chroma:
        """Carga, procesa y vectoriza los documentos."""
        print(f"ðŸ”„ Inicializando embeddings con: {MODELO_EMBEDDING}...")
        
        texto_base = """
Los clientes tienen 30 dÃ­as calendario para solicitar devoluciÃ³n.
El producto debe estar en condiciones comerciales.
GarantÃ­a por defectos de fabricaciÃ³n durante 12 meses.
El cliente debe presentar evidencia de compra.
El uso de la plataforma implica la aceptaciÃ³n de tÃ©rminos.
Las responsabilidades y limitaciones estÃ¡n descritas."""

        # OPTIMIZACIÃ“N 1: Chunking SemÃ¡ntico
        # Usamos separadores especÃ­ficos para no cortar oraciones legales a la mitad.
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=70, 
            chunk_overlap=0,
            separators=["\n", ".", ";"] 
        )
        
        docs = [Document(page_content=x.strip()) for x in text_splitter.split_text(texto_base) if x.strip()]

        # OPTIMIZACIÃ“N 2: Modelo MultilingÃ¼e
        embedding_function = HuggingFaceEmbeddings(model_name=MODELO_EMBEDDING)

        # Crear DB en memoria
        db = Chroma.from_documents(documents=docs, embedding=embedding_function)
        print("âœ… Base de conocimientos vectorizada y lista.")
        return db

    def consultar(self, pregunta: str) -> str:
        """Realiza la bÃºsqueda semÃ¡ntica y formatea la respuesta."""
        if not pregunta.strip():
            return "âš ï¸ Por favor, escribe una pregunta vÃ¡lida."

        # OPTIMIZACIÃ“N 3: BÃºsqueda con Score (Distancia)
        # k=3 para tener mÃ¡s contexto, pero filtraremos por calidad.
        resultados = self.vector_db.similarity_search_with_score(pregunta, k=3)

        contextos_validos = []
        
        for doc, score in resultados:
            # En Chroma (default L2), score bajo = mejor coincidencia.
            # Un score > 1.0 o 1.5 suele ser una coincidencia pobre para oraciones cortas.
            if score < UMBRAL_CORTE: 
                contextos_validos.append(f"â€¢ {doc.page_content} (Confianza: {1/score:.2f})")

        if not contextos_validos:
            return (
                "âŒ **InformaciÃ³n no encontrada.**\n\n"
                "El sistema no encontrÃ³ reglas relevantes en la base de conocimiento para tu consulta. "
                "Para garantizar la consistencia, no intentarÃ© inventar una respuesta."
            )

        # ConstrucciÃ³n del Prompt para el usuario (o para un futuro LLM)
        texto_contexto = "\n".join(contextos_validos)
        
        respuesta = (
            f"âœ… **InformaciÃ³n Recuperada (Base de Conocimiento):**\n\n"
            f"{texto_contexto}\n\n"
            f"---\n"
            f"ðŸ’¡ *Respuesta sugerida basada estrictamente en lo anterior:*\n"
            f"SegÃºn las polÃ­ticas: {contextos_validos[0].split('(')[0]}"
        )
        return respuesta

# Instancia global del motor
motor_rag = MotorRAG()

# --- INTERFAZ GRÃFICA ---
def interfaz_fn(pregunta):
    return motor_rag.consultar(pregunta)

tema_visual = gr.themes.Soft(
    primary_hue="blue",
    secondary_hue="slate",
    font=[gr.themes.GoogleFont("Inter"), "ui-sans-serif", "system-ui"]
)

with gr.Interface(
    fn=interfaz_fn,
    inputs=gr.Textbox(lines=2, placeholder="Ej: Â¿CuÃ¡nto tiempo tengo para devolver algo?", label="Consulta al Manual"),
    outputs=gr.Markdown(label="Respuesta Consistente"),
    title="ðŸ§¬ RAG Optimizer: Contexto Legal",
    description="Sistema de recuperaciÃ³n semÃ¡ntica optimizado para consistencia en espaÃ±ol.",
    theme=tema_visual,
    examples=[
        ["Â¿CuÃ¡nto dura la garantÃ­a?"],
        ["Â¿Puedo devolver el producto si ya lo usÃ©?"], # Pregunta capciosa para probar consistencia
        ["Â¿CÃ³mo contacto a soporte tÃ©cnico?"] # Pregunta fuera de dominio
    ]
) as demo:
    pass

if __name__ == "__main__":
    demo.launch()